{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29347c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Sets the path to the parent directory of RR classes\n",
    "os.chdir(r\"C:\\Users\\robso\\Projects\\Reproducible Research\\RRcourse2023\\6. Coding and documentation\")\n",
    "\n",
    "# Import data from the O*NET database, at ISCO-08 occupation level.\n",
    "# The original data uses a version of SOC classification, but the data we load here\n",
    "# are already cross-walked to ISCO-08 using: https://ibs.org.pl/en/resources/occupation-classifications-crosswalks-from-onet-soc-to-isco/\n",
    "\n",
    "# The O*NET database contains information for occupations in the USA, including\n",
    "# the tasks and activities typically associated with a specific occupation.\n",
    "\n",
    "task_data = pd.read_csv(\"Data\\\\onet_tasks.csv\")\n",
    "# isco08 variable is for occupation codes\n",
    "# the t_* variables are specific tasks conducted on the job\n",
    "\n",
    "# read employment data from Eurostat\n",
    "# These datasets include quarterly information on the number of workers in specific\n",
    "# 1-digit ISCO occupation categories. (Check here for details: https://www.ilo.org/public/english/bureau/stat/isco/isco08/)\n",
    "for i in range(1,10):\n",
    "    globals()[\"isco\"+str(i)] = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO\"+str(i))\n",
    "# isco1 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO1\")\n",
    "# isco2 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO2\")\n",
    "# isco3 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO3\")\n",
    "# isco4 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO4\")\n",
    "# isco5 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO5\")\n",
    "# isco6 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO6\")\n",
    "# isco7 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO7\")\n",
    "# isco8 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO8\")\n",
    "# isco9 = pd.read_excel(\"Data\\\\Eurostat_employment_isco.xlsx\", sheet_name=\"ISCO9\")\n",
    "\n",
    "# We will focus on three countries, but perhaps we could clean this code to allow it\n",
    "# to easily run for all the countries in the sample?\n",
    "\n",
    "# This will calculate worker totals in each of the chosen countries.\n",
    "for country in ['Belgium', 'Spain', 'Poland']:\n",
    "    globals()[\"total_\"+country] = 0\n",
    "    for i in range(1,10):\n",
    "        globals()[\"total_\"+country] += globals()[\"isco\"+str(i)][country]\n",
    "# total_Belgium = isco1[\"Belgium\"] + isco2[\"Belgium\"] + isco3[\"Belgium\"] + isco4[\"Belgium\"] + isco5[\"Belgium\"] + isco6[\"Belgium\"] + isco7[\"Belgium\"] + isco8[\"Belgium\"] + isco9[\"Belgium\"]\n",
    "# total_Spain = isco1[\"Spain\"] + isco2[\"Spain\"] + isco3[\"Spain\"] + isco4[\"Spain\"] + isco5[\"Spain\"] + isco6[\"Spain\"] + isco7[\"Spain\"] + isco8[\"Spain\"] + isco9[\"Spain\"]\n",
    "# total_Poland = isco1[\"Poland\"] + isco2[\"Poland\"] + isco3[\"Poland\"] + isco4[\"Poland\"] + isco5[\"Poland\"] + isco6[\"Poland\"] + isco7[\"Poland\"] + isco8[\"Poland\"] + isco9[\"Poland\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5543d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's merge all these datasets. We'll need a column that stores the occupation categories:\n",
    "for i in range(1,10):\n",
    "    globals()[\"isco\"+str(i)]['ISCO'] = i\n",
    "# isco1['ISCO'] = 1\n",
    "# isco2['ISCO'] = 2\n",
    "# isco3['ISCO'] = 3\n",
    "# isco4['ISCO'] = 4\n",
    "# isco5['ISCO'] = 5\n",
    "# isco6['ISCO'] = 6\n",
    "# isco7['ISCO'] = 7\n",
    "# isco8['ISCO'] = 8\n",
    "# isco9['ISCO'] = 9\n",
    "allDataList = []\n",
    "for i in range(1,10):\n",
    "    allDataList.append(globals()[\"isco\"+str(i)])\n",
    "# and this gives us one large file with employment in all occupations.\n",
    "all_data = pd.concat([isco1, isco2, isco3, isco4, isco5, isco6, isco7, isco8, isco9], ignore_index=True)\n",
    "\n",
    "# We have 9 occupations and the same time range for each, so we can add the totals by\n",
    "# adding a vector that is 9 times the previously calculated totals\n",
    "all_data[\"total_Belgium\"] = pd.concat([total_Belgium]*9, ignore_index=True)\n",
    "all_data[\"total_Spain\"] = pd.concat([total_Spain]*9, ignore_index=True)\n",
    "all_data[\"total_Poland\"] = pd.concat([total_Poland]*9, ignore_index=True)\n",
    "\n",
    "# And this will give us shares of each occupation among all workers in a period-country\n",
    "all_data['share_Belgium'] = all_data['Belgium'] / all_data['total_Belgium']\n",
    "all_data['share_Spain'] = all_data['Spain'] / all_data['total_Spain']\n",
    "all_data['share_Poland'] = all_data['Poland'] / all_data['total_Poland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the task data. We want the first digit of the ISCO variable only\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "task_data[\"isco08_1dig\"] = task_data[\"isco08\"].astype(str).str[:1].astype(int)\n",
    "\n",
    "# And we'll calculate the mean task values at a 1-digit level \n",
    "# (more on what these tasks are below)\n",
    "aggdata = task_data.groupby([\"isco08_1dig\"]).mean()\n",
    "aggdata = aggdata.drop(columns=[\"isco08\"])\n",
    "\n",
    "# We'll be interested in tracking the intensity of Non-routine cognitive analytical tasks\n",
    "# Using a framework reminiscent of the work by David Autor.\n",
    "\n",
    "#These are the ones we're interested in:\n",
    "# Non-routine cognitive analytical\n",
    "# 4.A.2.a.4 Analyzing Data or Information\n",
    "# 4.A.2.b.2 Thinking Creatively\n",
    "# 4.A.4.a.1 Interpreting the Meaning of Information for Others\n",
    "\n",
    "#Let's combine the data.\n",
    "combined = pd.merge(all_data, aggdata, left_on='ISCO', right_on='isco08_1dig', how='left')\n",
    "# Traditionally, the first step is to standardise the task values using weights \n",
    "# defined by share of occupations in the labour force. This should be done separately\n",
    "# for each country. Standardisation -> getting the mean to 0 and std. dev. to 1.\n",
    "# Let's do this for each of the variables that interests us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "#first task item\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2a4\"],weights=combined[\"share_Belgium\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2a4\"]-temp_mean)**2,weights=combined[\"share_Belgium\"]))\n",
    "combined[\"std_Belgium_t_4A2a4\"]=(combined[\"t_4A2a4\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2a4\"],weights=combined[\"share_Poland\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2a4\"]-temp_mean)**2,weights=combined[\"share_Poland\"]))\n",
    "combined[\"std_Poland_t_4A2a4\"]=(combined[\"t_4A2a4\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2a4\"],weights=combined[\"share_Spain\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2a4\"]-temp_mean)**2,weights=combined[\"share_Spain\"]))\n",
    "combined[\"std_Spain_t_4A2a4\"]=(combined[\"t_4A2a4\"]-temp_mean)/temp_sd\n",
    "#second task item\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2b2\"],weights=combined[\"share_Belgium\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2b2\"]-temp_mean)**2,weights=combined[\"share_Belgium\"]))\n",
    "combined[\"std_Belgium_t_4A2b2\"]=(combined[\"t_4A2b2\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2b2\"],weights=combined[\"share_Poland\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2b2\"]-temp_mean)**2,weights=combined[\"share_Poland\"]))\n",
    "combined[\"std_Poland_t_4A2b2\"]=(combined[\"t_4A2b2\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A2b2\"],weights=combined[\"share_Spain\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A2b2\"]-temp_mean)**2,weights=combined[\"share_Spain\"]))\n",
    "combined[\"std_Spain_t_4A2b2\"]=(combined[\"t_4A2b2\"]-temp_mean)/temp_sd\n",
    "\n",
    "#third task item\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A4a1\"],weights=combined[\"share_Belgium\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A4a1\"]-temp_mean)**2,weights=combined[\"share_Belgium\"]))\n",
    "combined[\"std_Belgium_t_4A4a1\"]=(combined[\"t_4A4a1\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A4a1\"],weights=combined[\"share_Poland\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A4a1\"]-temp_mean)**2,weights=combined[\"share_Poland\"]))\n",
    "combined[\"std_Poland_t_4A4a1\"]=(combined[\"t_4A4a1\"]-temp_mean)/temp_sd\n",
    "\n",
    "temp_mean=np.average(combined[\"t_4A4a1\"],weights=combined[\"share_Spain\"])\n",
    "temp_sd=np.sqrt(np.average((combined[\"t_4A4a1\"]-temp_mean)**2,weights=combined[\"share_Spain\"]))\n",
    "combined[\"std_Spain_t_4A4a1\"]=(combined[\"t_4A4a1\"]-temp_mean)/temp_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b853c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to calculate the `classic` task content intensity, i.e.\n",
    "# how important is a particular general task content category in the workforce\n",
    "# Here, we're looking at non-routine cognitive analytical tasks, as defined\n",
    "# by David Autor and Darron Acemoglu:\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "combined[\"Belgium_NRCA\"] = combined[\"std_Belgium_t_4A2a4\"] + combined[\"std_Belgium_t_4A2b2\"] + combined[\"std_Belgium_t_4A4a1\"]\n",
    "combined[\"Poland_NRCA\"] = combined[\"std_Poland_t_4A2a4\"] + combined[\"std_Poland_t_4A2b2\"] + combined[\"std_Poland_t_4A4a1\"]\n",
    "combined[\"Spain_NRCA\"] = combined[\"std_Spain_t_4A2a4\"] + combined[\"std_Spain_t_4A2b2\"] + combined[\"std_Spain_t_4A4a1\"]\n",
    "\n",
    "# And we standardise NRCA in a similar way.\n",
    "temp_mean = np.average(combined[\"Belgium_NRCA\"], weights=combined[\"share_Belgium\"])\n",
    "temp_sd = np.sqrt(np.average((combined[\"Belgium_NRCA\"] - temp_mean)**2, weights=combined[\"share_Belgium\"]))\n",
    "combined[\"std_Belgium_NRCA\"] = (combined[\"Belgium_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "temp_mean = np.average(combined[\"Poland_NRCA\"], weights=combined[\"share_Poland\"])\n",
    "temp_sd = np.sqrt(np.average((combined[\"Poland_NRCA\"] - temp_mean)**2, weights=combined[\"share_Poland\"]))\n",
    "combined[\"std_Poland_NRCA\"] = (combined[\"Poland_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "temp_mean = np.average(combined[\"Spain_NRCA\"], weights=combined[\"share_Spain\"])\n",
    "temp_sd = np.sqrt(np.average((combined[\"Spain_NRCA\"] - temp_mean)**2, weights=combined[\"share_Spain\"]))\n",
    "combined[\"std_Spain_NRCA\"] = (combined[\"Spain_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "\n",
    "# Finally, to track the changes over time, we have to calculate a country-level mean\n",
    "# Step 1: multiply the value by the share of such workers.\n",
    "combined[\"multip_Spain_NRCA\"] = combined[\"std_Spain_NRCA\"] * combined[\"share_Spain\"]\n",
    "combined[\"multip_Belgium_NRCA\"] = combined[\"std_Belgium_NRCA\"] * combined[\"share_Belgium\"]\n",
    "combined[\"multip_Poland_NRCA\"] = combined[\"std_Poland_NRCA\"] * combined[\"share_Poland\"]\n",
    "\n",
    "# Step 2: sum it up (it basically becomes another weighted mean)\n",
    "\n",
    "agg_Spain = combined.groupby([\"TIME\"])[\"multip_Spain_NRCA\"].sum().reset_index()\n",
    "agg_Belgium = combined.groupby([\"TIME\"])[\"multip_Belgium_NRCA\"].sum().reset_index()\n",
    "agg_Poland = combined.groupby([\"TIME\"])[\"multip_Poland_NRCA\"].sum().reset_index()\n",
    "\n",
    "# We can plot it now!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agg_Poland[\"TIME\"], agg_Poland[\"multip_Poland_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Poland), 3), agg_Poland[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(agg_Spain[\"TIME\"], agg_Spain[\"multip_Spain_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Spain), 3), agg_Spain[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(agg_Belgium[\"TIME\"], agg_Belgium[\"multip_Belgium_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Belgium), 3), agg_Belgium[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "# If this code gets automated and cleaned properly,\n",
    "#  you should be able to easily add other countries as well as other tasks.\n",
    "# E.g.:\n",
    "\n",
    "# Routine manual\n",
    "# 4.A.3.a.3\tControlling Machines and Processes\n",
    "# 4.C.2.d.1.i\tSpend Time Making Repetitive Motions\n",
    "# 4.C.3.d.3\tPace Determined by Speed of Equipment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e70f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
